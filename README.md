# Project outline

Each video saved inside `"results/video"` directory which contain one video for each environment and each algorithm only best strategy using gymnasium package.

In the `weights` folder have trained file `1000.pth`, `2000.pth`, `best.pth` and `training.log` for each algorithm and each environment.

`plots_images` folder contain all visualization `7 images` and a `checkpoint_summary.csv` file for each environments and algorithms

## Installation

Please follow these steps to install the necessary dependencies and set up the project locally. To see required packages please open the `requirements.txt` file.

### 1. Clone the repository:

```bash
git clone https://github.com/raselmahmud-coder/RL_Experiment_2.git
cd RL_Experiment_2
pip install -r requirements.txt
```

### For Training the Project:
In this project have 3 algorithms `"DQN", "DoubleDQN", "DuelingDQN"` and 3 environments `"CartPole-v1", "MountainCar-v0", "LunarLander-v3"`

You need to change the algorithm and environment argument for sequential training.

```bash
python main.py --algorithm DQN --environment CartPole-v1     
```

### For Visualization the Project:
We have 3 environment here:
- "CartPole-v1"
- "MountainCar-v0"
- "LunarLander-v3"


For visual and compare you need to set i.e., `env_name = 'LunarLander-v3'` manually need to change for each environment name and then run below command it will save specific folder each algorithms plot.

```bash
python .\results\visualize_comparison.py    
python .\results\compare_checkpoints.py    
```


## File Hierarchy
```bash
ğŸ“¦RL_Experiment_2
 â”£ ğŸ“‚results
 â”ƒ â”£ ğŸ“‚plots_images
 â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1_ENV
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcheckpoint_comparison.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcheckpoint_summary.csv
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcombined_metrics.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcomparison_epsilon_decay.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcomparison_rewards.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvergence_speed.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œlearning_progress.png
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œstability_rewards.png
 â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3_ENV
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcheckpoint_comparison.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcheckpoint_summary.csv
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcombined_metrics.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcomparison_epsilon_decay.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcomparison_rewards.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvergence_speed.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œlearning_progress.png
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œstability_rewards.png
 â”ƒ â”ƒ â”£ ğŸ“‚MountainCar-v0_ENV
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcheckpoint_comparison.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcheckpoint_summary.csv
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcombined_metrics.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcomparison_epsilon_decay.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œcomparison_rewards.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvergence_speed.png
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œlearning_progress.png
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œstability_rewards.png
 â”ƒ â”ƒ â”£ ğŸ“œDDQN_Alog.png
 â”ƒ â”ƒ â”£ ğŸ“œdqn_algo.png
 â”ƒ â”ƒ â”£ ğŸ“œDueling_dqn.png
 â”ƒ â”ƒ â”— ğŸ“œmodel_code_snippet.jpeg
 â”ƒ â”£ ğŸ“‚videos
 â”ƒ â”ƒ â”£ ğŸ“‚DoubleDQN
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”£ ğŸ“‚DQN
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”— ğŸ“‚DuelingDQN
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œbest_strategy.mp4
 â”ƒ â”£ ğŸ“‚weights
 â”ƒ â”ƒ â”£ ğŸ“‚DoubleDQN
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”£ ğŸ“‚DQN
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”— ğŸ“‚DuelingDQN
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”— ğŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œtraining.log
 â”ƒ â”£ ğŸ“œcompare_checkpoints.py
 â”ƒ â”— ğŸ“œvisualize_comparison.py
 â”£ ğŸ“‚utils
 â”ƒ â”£ ğŸ“œlogger.py
 â”ƒ â”— ğŸ“œvideo_recorder.py
 â”£ ğŸ“œbase_dqn.py
 â”£ ğŸ“œdata.py
 â”£ ğŸ“œdouble_dqn.py
 â”£ ğŸ“œdqn.py
 â”£ ğŸ“œdueling_dqn.py
 â”£ ğŸ“œmain.py
 â”£ ğŸ“œmemory.py
 â”£ ğŸ“œmodels.py