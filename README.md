# Deep-Q-Network DQN Variants Analysis for Optimization

### Programmatically recorded the best strategy's videos for each algorithm and each environment.


*CartPole Demo Dueling Model*

![CartPole Demo Dueling](/Results/videos/DuelingDQN/CartPole-v1/CartPole_best_strategy.gif)

*Lunar lander Demo Dueling Model*

![Lunar lander Demo Dueling](/Results/videos/DuelingDQN/LunarLander-v3/Luner_best_strategy.gif)

*Mountain Car Demo Dueling Model*

![Mountain Car Demo Dueling](/Results/videos/DuelingDQN/MountainCar-v0/Mountain_best_strategy.gif)


## Analysis result summary:
![Alt Text](/Results/summary_result.png)


### My Presentation PPT: [Click here to see the full presentation](https://www.canva.com/design/DAGahDwGwko/1xpOAfOGUGTl46n14YIHnA/view?utm_content=DAGahDwGwko&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=hdc0cf455e3)

## Project Overview
Each video saved inside `"results/video"` directory which contain one video for each environment and each algorithm only best strategy using gymnasium package.

In the `weights` directory have trained file `1000.pth`, `2000.pth`, `best.pth` and `training.log` for each algorithm and each environment.

`plots_images` directory contain all visualization `7 images` and a `checkpoint_summary.csv` file for each environments and algorithms

## Installation

Please follow these steps to install the necessary dependencies and set up the project locally. To see required packages please open the `requirements.txt` file.

### 1. Clone the repository:

```bash
git clone https://github.com/raselmahmud-coder/RL_Experiment_2.git
cd RL_Experiment_2
pip install -r requirements.txt
```

### For Training the Project:
In this project have 3 algorithms `"DQN", "DoubleDQN", "DuelingDQN"` and 3 environments `"CartPole-v1", "MountainCar-v0", "LunarLander-v3"`

You need to change the algorithm and environment argument for sequential training.

```bash
python main.py --algorithm DQN --environment CartPole-v1     
```

### For Visualization the Project:
We have 3 environment here:
- "CartPole-v1"
- "MountainCar-v0"
- "LunarLander-v3"


For visual and compare you need to set i.e., `env_name = 'LunarLander-v3'` manually need to change for each environment name and then run below command it will save specific directory each algorithms plot.

```bash
python .\results\visualize_comparison.py    
python .\results\compare_checkpoints.py    
```


## File Hierarchy
```bash
ðŸ“¦RL_Experiment_2
 â”£ ðŸ“‚results
 â”ƒ â”£ ðŸ“‚plots_images
 â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1_ENV
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcheckpoint_comparison.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcheckpoint_summary.csv
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcombined_metrics.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcomparison_epsilon_decay.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcomparison_rewards.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œconvergence_speed.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œlearning_progress.png
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œstability_rewards.png
 â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3_ENV
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcheckpoint_comparison.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcheckpoint_summary.csv
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcombined_metrics.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcomparison_epsilon_decay.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcomparison_rewards.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œconvergence_speed.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œlearning_progress.png
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œstability_rewards.png
 â”ƒ â”ƒ â”£ ðŸ“‚MountainCar-v0_ENV
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcheckpoint_comparison.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcheckpoint_summary.csv
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcombined_metrics.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcomparison_epsilon_decay.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œcomparison_rewards.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œconvergence_speed.png
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œlearning_progress.png
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œstability_rewards.png
 â”ƒ â”ƒ â”£ ðŸ“œDDQN_Alog.png
 â”ƒ â”ƒ â”£ ðŸ“œdqn_algo.png
 â”ƒ â”ƒ â”£ ðŸ“œDueling_dqn.png
 â”ƒ â”ƒ â”— ðŸ“œmodel_code_snippet.jpeg
 â”ƒ â”£ ðŸ“‚videos
 â”ƒ â”ƒ â”£ ðŸ“‚DoubleDQN
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”£ ðŸ“‚DQN
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”— ðŸ“‚DuelingDQN
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œbest_strategy.mp4
 â”ƒ â”£ ðŸ“‚weights
 â”ƒ â”ƒ â”£ ðŸ“‚DoubleDQN
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”£ ðŸ“‚DQN
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”— ðŸ“‚DuelingDQN
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚CartPole-v1
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚LunarLander-v3
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚MountainCar-v0
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ1000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ2000.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œbest.pth
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œtraining.log
 â”ƒ â”£ ðŸ“œcompare_checkpoints.py
 â”ƒ â”— ðŸ“œvisualize_comparison.py
 â”£ ðŸ“‚utils
 â”ƒ â”£ ðŸ“œlogger.py
 â”ƒ â”— ðŸ“œvideo_recorder.py
 â”£ ðŸ“œbase_dqn.py
 â”£ ðŸ“œdata.py
 â”£ ðŸ“œdouble_dqn.py
 â”£ ðŸ“œdqn.py
 â”£ ðŸ“œdueling_dqn.py
 â”£ ðŸ“œmain.py
 â”£ ðŸ“œmemory.py
 â”£ ðŸ“œmodels.py
